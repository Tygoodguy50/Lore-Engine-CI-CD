# ðŸŽ¯ LocalAI Model Configuration
models:
  - name: phi-2
    backend: llama
    parameters:
      model: phi-2
      context_size: 2048
      threads: 4
      f16: true
      
  - name: code-llama
    backend: llama
    parameters:
      model: code-llama-7b
      context_size: 4096
      threads: 6
      
  - name: embedding
    backend: bert
    parameters:
      model: all-MiniLM-L6-v2
      pooling: mean
